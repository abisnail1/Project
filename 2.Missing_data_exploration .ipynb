{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file will go thorugh the data and explore the missing values\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import missingno as msno\n",
    "import calendar\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from pyrealm import pmodel\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import pycountry_convert as pc\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dfs from the data_processing file\n",
    "\n",
    "full_date=pd.read_csv('/Users/abigailbase/PROJECT FILES/FINAL DFs/full_date.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7260f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_date.head() #inspect the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce404203",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_date.shape) #580386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92becd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import FLUXNET tower site information \n",
    "# source: https://fluxnet.org/sites/site-list-and-pages/\n",
    "\n",
    "site_key=pd.read_csv('/Users/abigailbase/PROJECT FILES/site_key.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f4f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare for left join\n",
    "    \n",
    "site_key_merge=site_key.drop(columns='SITE_NAME') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417238e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the FLUXNET data and the site info dfs\n",
    "\n",
    "final_df=pd.merge(full_date,site_key_merge,on='SITE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a028edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head() #inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a171e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign hemisphere to all points in the merged df\n",
    "\n",
    "final_df['hemisphere']=final_df['LAT'].apply(lambda x: 'NH' if x>=0 else 'SH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e85fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by hemisphere and year for anaylsis of observations over time\n",
    "\n",
    "year_hs_count=final_df.groupby(['hemisphere','YEAR',]).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64592ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate by hemisphere\n",
    "\n",
    "NH_year=year_hs_count[year_hs_count['hemisphere']=='NH']\n",
    "SH_year=year_hs_count[year_hs_count['hemisphere']=='SH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980bea3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot of observations overtime per hemisphere with NH on primary axis and SH on secondary axis \n",
    "\n",
    "fig, ax3 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "\n",
    "ax3.set_xlabel('Year',fontsize=13)\n",
    "ax3.set_ylabel('Observations in Northern Hemisphere',fontsize=13)\n",
    "NH_line2=ax3.plot(NH_year['YEAR'], NH_year['counts'], \n",
    "                  color='blue', marker='o', \n",
    "                  label='Northern Hemisphere')\n",
    "\n",
    "ax3.tick_params(axis='y')\n",
    "ax3.tick_params(axis='y', labelsize=12) \n",
    "ax3.tick_params(axis='x', labelsize=12)\n",
    "\n",
    "ax4 = ax3.twinx()\n",
    "ax4.set_ylabel('Observations in Southern Hemisphere',fontsize=13)\n",
    "SH_line2=ax4.plot(SH_year['YEAR'], SH_year['counts'], color='red', marker='s', label='Southern Hemisphere')\n",
    "ax4.tick_params(axis='y')\n",
    "ax4.tick_params(axis='y', labelsize=12)  \n",
    "\n",
    "lines2 = NH_line2 + SH_line2\n",
    "labels2 = [line.get_label() for line in lines2]\n",
    "ax3.legend(lines2, labels2, loc='upper left',fontsize=12.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25207351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pre 2000 data due to low count\n",
    "\n",
    "post_2000=final_df[final_df['YEAR']>=2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c23fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(post_2000.shape) #198678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fed63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "post_2000.isna().sum() #No missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a633f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_2000=post_2000.drop(columns='PPFD_OUT') #drop PPFD out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a8b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the different land cover types ahead of stratified sampling\n",
    "# to ensure they are accuratly represented in the sample.\n",
    "\n",
    "total_ID= post_2000.groupby('SITE_ID').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge sites information with the counts\n",
    "\n",
    "site_counts=pd.merge(site_key,total_ID,on='SITE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional land cover information (IGBP codes)\n",
    "# source: https://fluxnet.org/data/badm-data-templates/igbp-classification/\n",
    "\n",
    "IGBP=pd.read_csv('/Users/abigailbase/PROJECT FILES/IGBP_code.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign hemispheres to the site locations\n",
    "\n",
    "site_counts['hemisphere']=site_counts['LAT'].apply(lambda x: 'NH' if x>=0 else 'SH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a6abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group counts by hemisphere \n",
    "\n",
    "land_counts=site_counts.groupby(['hemisphere','IGBP',]).size().reset_index(name='counts')\n",
    "\n",
    "NH_land=land_counts[land_counts['hemisphere']=='NH']\n",
    "SH_land=land_counts[land_counts['hemisphere']=='SH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16499224",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SH_land['counts'].sum()) #29\n",
    "print(NH_land['counts'].sum()) #176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ce867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on the common IGBP types\n",
    "\n",
    "NH_land = NH_land.rename(columns={'counts': 'NH_count'})\n",
    "SH_land = SH_land.rename(columns={'counts': 'SH_count'})\n",
    "\n",
    "# Merge the dataframes on 'IGBP'\n",
    "combined_bar = pd.merge(NH_land[['IGBP', 'NH_count']], SH_land[['IGBP', 'SH_count']], on='IGBP', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with 0 \n",
    "\n",
    "combined_bar = combined_bar.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGBP types to drop that dont appear in the df\n",
    "\n",
    "to_drop=['BSV','CVM','DNF','URB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6595c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows containing the IGBPS that dont appear\n",
    "\n",
    "IGBP = IGBP[~IGBP['IGBP'].isin(to_drop)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa29fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_labels = {'NH_count': 'Northern Hemisphere', 'SH_count': 'Southern Hemisphere'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fcb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "### grouped bar chart showing the IGBP types that appear in the df split by hemisphere\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# width for the bars\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(combined_bar['IGBP']))\n",
    "\n",
    "# plot bars for NH and SH\n",
    "bars1 = ax.bar(index, combined_bar['NH_count'], bar_width, label='NH_count',color='royalblue',edgecolor='black')\n",
    "bars2 = ax.bar(index + bar_width, combined_bar['SH_count'], bar_width, label='SH_count',color='orangered',edgecolor='black')\n",
    "\n",
    "ax.set_xlabel(('IGBP'),fontsize=14)\n",
    "ax.set_ylabel(('Counts'),fontsize=14)\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(combined_bar['IGBP'])\n",
    "\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "legend_labels = {'NH_count': 'NH', 'SH_count': 'SH'}\n",
    "\n",
    "new_labels = [legend_labels[label] for label in labels]\n",
    "\n",
    "ax.legend(handles, new_labels, fontsize=12)\n",
    "\n",
    "# add labels above the bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    if np.isfinite(height):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2.0, height, f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    if np.isfinite(height):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2.0, height, f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "legend_elements = []\n",
    "for idx, row in IGBP.iterrows():\n",
    "    legend_elements.append(plt.Line2D([0], [0], color='black', lw=0, label=f\"{row['IGBP']}= {row['Desc']}\"))\n",
    "        \n",
    "fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3, frameon=True,fontsize=12)\n",
    "\n",
    "        \n",
    "plt.xticks()  \n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607db6e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b5ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to perform a straisfied sample on the entire df to lower the selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd27859",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat1=post_2000.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a3c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_2000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by hemisphere\n",
    "\n",
    "NH1=strat1[strat1['hemisphere']=='NH']\n",
    "SH1=strat1[strat1['hemisphere']=='SH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fc2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate proportion of NH and SH in df to ensure accurate representation \n",
    "\n",
    "nh_prop=len(NH1)/len(strat1)\n",
    "sh_prop=len(SH1)/len(strat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c12e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller stratified sample to reduce comp demand. \n",
    "\n",
    "total_size=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc001d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_sample_size = int(total_size * nh_prop)\n",
    "sh_sample_size = total_size - nh_sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for perfomring stratified sample\n",
    "\n",
    "def stratified_sample(data, strata_column, sample_size):\n",
    "    data = data.copy()  # Ensure we are working with a copy of the data\n",
    "    data['strata'] = data[strata_column]\n",
    "    test_size = sample_size / len(data)\n",
    "    \n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "    \n",
    "    for train_index, test_index in split.split(data, data['strata']):\n",
    "        sample = data.iloc[test_index]\n",
    "    \n",
    "    sample = sample.drop(columns=['strata'])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take samples independantly \n",
    "\n",
    "nh_sample = stratified_sample(NH1, 'IGBP', nh_sample_size)\n",
    "sh_sample = stratified_sample(SH1, 'IGBP', sh_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e693db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the strat sample selected 47 random sites i want to now reduce this\n",
    "# to 15 sites whilst maintaing the proportion \n",
    "\n",
    "num_nh = len(nh_sample)\n",
    "num_sh = len(sh_sample)\n",
    "\n",
    "total_towers=num_nh+num_sh\n",
    "\n",
    "nh_proportion = num_nh/ total_towers\n",
    "sh_proportion = num_sh / total_towers\n",
    "\n",
    "print(f\"Proportion of NH towers: {nh_proportion:.2f}\")\n",
    "print(f\"Proportion of SH towers: {sh_proportion:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ce7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_total_towers = 15\n",
    "\n",
    "# calculate the number of towers to select from each hemisphere\n",
    "num_nh_selected = round(desired_total_towers * nh_proportion)\n",
    "num_sh_selected = desired_total_towers - num_nh_selected  # Ensure the total adds up to the desired number\n",
    "\n",
    "print(f\"Number of NH towers to select: {num_nh_selected}\") #12\n",
    "print(f\"Number of SH towers to select: {num_sh_selected}\") # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now randomly select from each hemisphere\n",
    "\n",
    "nh_towers=nh_sample['SITE_ID'].unique()\n",
    "sh_towers=sh_sample['SITE_ID'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=2020\n",
    "np.random.seed(seed)\n",
    "\n",
    "selected_nh=np.random.choice(nh_towers,num_nh_selected,replace=False)\n",
    "selected_sh=np.random.choice(sh_towers,num_sh_selected,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc36d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_nh=['BE-Vie','CA-TP1','CH-Cha','DE-Gri','FR-Pue','GF-Guy','IT-Col','NL-Loo',\n",
    "            'RU-Cok','RU-Fyo','US-Pfa','US-Var']\n",
    "selected_sh=['AR-Vir','AU-Dry','ZA-Kru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bf83e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for mapping\n",
    "\n",
    "nh_sites_plot = site_key[site_key['SITE_ID'].isin(selected_nh)]\n",
    "sh_sites_plot = site_key[site_key['SITE_ID'].isin(selected_sh)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b467084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom positions of the labels so they dont overlap \n",
    "\n",
    "label_offsets={\n",
    "    'BE-Vie\t':(5,10),\n",
    "    'CA-TP1':(10,10),\n",
    "    'CH-Cha':(-30,0),\n",
    "    'DE-Gri':(18,-5),\n",
    "    'FR-Pue':(-20,-10),\n",
    "    'GF-Guy':(10,10),\n",
    "    'IT-Col':(10,-15),\n",
    "    'NL-Loo':(-25,5),\n",
    "    'RU-Cok':(10,10),\n",
    "    'RU-Fyo':(10,5),\n",
    "    'US-PFa':(10,20),\n",
    "    'US-Var':(-25,10)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2626bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of final sites\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(22, 14))\n",
    "\n",
    "final_map = Basemap(projection='cyl', llcrnrlat=-90, urcrnrlat=90,\n",
    "                    llcrnrlon=-180, urcrnrlon=180, resolution='c', ax=ax)\n",
    "\n",
    "final_map.drawcoastlines()\n",
    "\n",
    "# add the sites\n",
    "final_map.scatter(nh_sites_plot['LONG'], nh_sites_plot['LAT'], marker='v', color='blue', edgecolor='black', s=100,alpha=1,zorder=5)\n",
    "final_map.scatter(sh_sites_plot['LONG'], sh_sites_plot['LAT'], marker='v', color='red', edgecolor='black', s=100,alpha=1,zorder=5)\n",
    "\n",
    "\n",
    "# plot  'US-Var'\n",
    "final_map.scatter(\n",
    "    nh_sites_plot[nh_sites_plot['SITE_ID'] == 'US-Var']['LONG'], \n",
    "    nh_sites_plot[nh_sites_plot['SITE_ID'] == 'US-Var']['LAT'],\n",
    "    marker='d', color='yellow', edgecolor='red',lw=2, s=120, alpha=1, zorder=6, label='Model Validation Site'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# add lines to map \n",
    "parallels = np.arange(-90., 91., 30.)\n",
    "meridians = np.arange(-180., 181., 60.)\n",
    "final_map.drawparallels(parallels, labels=[1, 0, 0, 0], linewidth=0.5, color='grey')\n",
    "final_map.drawmeridians(meridians, labels=[0, 0, 0, 1], linewidth=0.5, color='grey')\n",
    "\n",
    "# points for legend\n",
    "dummy_scatter_nh = plt.scatter([], [], color='blue', marker='v', alpha=1, s=180, edgecolors='black', label='NH Site')\n",
    "dummy_scatter_sh = plt.scatter([], [], color='red', marker='v', s=180, edgecolors='black', label='SH Site')\n",
    "dummy_scatter_validation = plt.scatter([], [], color='yellow', marker='D', s=120, edgecolors='red', label='Validation Site')\n",
    "\n",
    "\n",
    "legend = plt.legend(handles=[dummy_scatter_nh, dummy_scatter_sh,dummy_scatter_validation], loc='lower left', fontsize=17)\n",
    "\n",
    "frame = legend.get_frame()\n",
    "frame.set_edgecolor('black')      \n",
    "frame.set_linewidth(1.5)          \n",
    "frame.set_alpha(1)\n",
    "\n",
    "# function for adding sites labels with the custom offsets\n",
    "def add_labels(sites, color):\n",
    "    for idx, row in sites.iterrows():\n",
    "        offset = label_offsets.get(row['SITE_ID'], (10, 10))  # Default offset if not in dictionary\n",
    "        plt.annotate(\n",
    "            row['SITE_ID'],\n",
    "            xy=(row['LONG'], row['LAT']),\n",
    "            xytext=(row['LONG'] + offset[0], row['LAT'] + offset[1]),\n",
    "            fontsize=14,\n",
    "            fontweight='bold',\n",
    "            color='white',\n",
    "            arrowprops=dict(arrowstyle='-', color='grey', lw=2),\n",
    "            bbox=dict(boxstyle=\"round,pad=0.35\", edgecolor='black', facecolor='dimgrey',lw=2)\n",
    "        )\n",
    "\n",
    "# add labels\n",
    "add_labels(nh_sites_plot, 'black') # NH\n",
    "add_labels(sh_sites_plot, 'black') # SH \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract coords for downloading additional \n",
    "\n",
    "\n",
    "nh_sites_coord=nh_sites_plot[['SITE_ID','LAT','LONG']]\n",
    "sh_sites_coord=sh_sites_plot[['SITE_ID','LAT','LONG']]\n",
    "\n",
    "all_sites_coord = pd.concat([nh_sites_coord, sh_sites_coord], ignore_index=True)\n",
    "all_sites_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the  ds to the selected sites \n",
    "\n",
    "final_nh=post_2000[post_2000['SITE_ID'].isin(selected_nh)]\n",
    "final_sh=post_2000[post_2000['SITE_ID'].isin(selected_sh)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c627d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine NH and SH sites\n",
    "\n",
    "selected_sites=np.concatenate([selected_nh,selected_sh]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df=pd.concat([final_nh,final_sh],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f3628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter the dataset to only be post 2010\n",
    "\n",
    "combined_df=combined_df[combined_df['YEAR']>=2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea40c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape #(25930, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['YEAR'].unique() #double check the filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c2ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract year, month, day\n",
    "\n",
    "combined_df['date'] = pd.to_datetime(combined_df[['YEAR', 'MONTH', 'DAY']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### -9999 is an instrument error so this needs to be replaced with NA ###\n",
    "\n",
    "total_negative_9999 = (combined_df == -9999).sum().sum()\n",
    "print(total_negative_9999) #9965 error (-9999) values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2478b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.replace(-9999,np.nan,inplace=True) # replace with NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d25a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new df of the entire data range to check for missing days\n",
    "\n",
    "\n",
    "date_range = pd.date_range(start='2010-01-01', end='2014-12-31', freq='D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf54c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "\n",
    "# loop through the selected site_ids and create df with all dates for each site\n",
    "\n",
    "for site_id in selected_sites:\n",
    "    df = pd.DataFrame(date_range, columns=['date'])\n",
    "    df['SITE_ID'] = site_id\n",
    "    dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8bb8b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concat the dfs together\n",
    "\n",
    "all_dates_df = pd.concat(dataframes, ignore_index=True)\n",
    "all_dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce0ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged df of the 15 sites and the entire date range including NA \n",
    "# on missing values that need imputing\n",
    "\n",
    "merged=pd.merge(combined_df,all_dates_df,on=['SITE_ID','date'],how='right',indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e298ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date to pd datetime format and set as the index\n",
    "\n",
    "merged['date'] = pd.to_datetime(merged['date'])\n",
    "merged.set_index('date', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e9795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=merged.drop(columns=['_merge']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fcad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop non-predictive variables from the df for heatmap \n",
    "\n",
    "missing_df=merged.drop(columns=['SITE_ID','YEAR','MONTH','DAY','LAT','LONG','IGBP',\n",
    "                        'hemisphere','NEE_VUT_REF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89165610",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom labels\n",
    "\n",
    "var_ax_labs = {\n",
    "    'TA_F': r'Temperature (°C)',\n",
    "    'PA_F': r'Surface Pressure (kPa)',\n",
    "    'VPD_F': r'VPD (hPa)',\n",
    "    'P_F': r'Precipitation (mm)',\n",
    "    'WS_F': r'WS (ms$^{-1}$)',\n",
    "    'PPFD_IN': r'PPFD ($\\mu$mol Photon m$^{-2}$ s$^{-1}$)',\n",
    "    'CO2_F_MDS': r'CO$_2$ MF ($\\mu$mol CO$_2$ mol$^{-1}$)',\n",
    "    'TS_F_MDS_1': r'ST (°C)',\n",
    "    'SWC_F_MDS_1': r'SWC (%)',\n",
    "    'GPP_DT_VUT_REF': r'GPP (gC m$^{-2}$ d$^{-1}$)'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_var_labs = [var_ax_labs.get(col, col) for col in missing_df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa70a0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot heatmap of the missing data\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "ax = sns.heatmap(missing_df.isna().transpose(), cbar=False, cmap='YlOrRd',\n",
    "                 yticklabels=ordered_var_labs)  # Apply labels directly here\n",
    "\n",
    "xticks = np.arange(0, len(missing_df.index), 1500)  # Adjust interval as needed\n",
    "\n",
    "\n",
    "\n",
    "# remove time aspect of the date from the x-axis\n",
    "ax.set_xticks(np.arange(len(missing_df.index))[::1500])  # Show only one tick every 1500 entries\n",
    "ax.set_xticklabels(missing_df.index.strftime('%Y-%m-%d')[::1500], rotation=45, ha='right')\n",
    "\n",
    "plt.xlabel('Date', fontsize=14) \n",
    "plt.ylabel('Variables', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf8f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DataFrame Columns:\")\n",
    "print(missing_df.columns)\n",
    "print(\"\\nOrdered Labels:\")\n",
    "print(var_ax_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts=merged.isna().sum() #No missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aba6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work out the counts and percentage of missing values\n",
    "\n",
    "total_count=len(merged)\n",
    "total_count #27390\n",
    "\n",
    "perc_missing=(missing_counts/total_count)*100\n",
    "\n",
    "missing_percs=pd.DataFrame({'count':missing_counts,'percentage':perc_missing})\n",
    "\n",
    "missing_percs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in missing years, hemisphere, lat lon and IGBP\n",
    "\n",
    "merged.reset_index(inplace=True) # reset index to extract date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd97721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redo the date extraction to include the new dates\n",
    "\n",
    "merged['YEAR']=merged['date'].dt.year\n",
    "merged['MONTH']=merged['date'].dt.month\n",
    "merged['DAY']=merged['date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e910c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.isna().sum() #No missing values for dates anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c580a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now to fill in the missing IGBP and lat lon from site_key then\n",
    "#can apply the hemisphere function to fill those\n",
    "\n",
    "def fill_missing(row, key_df):\n",
    "    \n",
    "    site_id = row['SITE_ID']\n",
    "    \n",
    "    if pd.isna(row['LAT']):\n",
    "        row['LAT'] = site_key.loc[site_key['SITE_ID'] == row['SITE_ID'], 'LAT'].values[0]\n",
    "    if pd.isna(row['LONG']):\n",
    "        row['LONG'] = site_key.loc[site_key['SITE_ID'] == row['SITE_ID'], 'LONG'].values[0]\n",
    "    if pd.isna(row['IGBP']):\n",
    "        row['IGBP']=site_key.loc[site_key['SITE_ID']==row['SITE_ID'], 'IGBP'].values[0]\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f630000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#apply the function to fill in the missing values from site_key\n",
    "\n",
    "final_df=merged.apply(fill_missing,axis=1,key_df=site_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.isna().sum() #values successfully imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c421f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the previous hemisphere function to fillt those missing value\n",
    "\n",
    "final_df['hemisphere']=final_df['LAT'].apply(lambda x: 'NH' if x>=0 else 'SH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab1dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_missing=final_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c302925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to create a filled bar chart to show the missing data by site\n",
    "\n",
    "\n",
    "#group missing data by site_id\n",
    "sites_missing_counts = site_missing.groupby('SITE_ID').apply(lambda x: x.isnull().sum().sum()).reset_index()\n",
    "\n",
    "sites_missing_counts.rename(columns={0: 'counts'}, inplace=True)\n",
    "\n",
    "# add missing gpp values\n",
    "site_missing['gpp_missing']=site_missing['GPP_DT_VUT_REF'].isna() \n",
    "\n",
    "# calculate missing gpp counts\n",
    "gpp_missing_counts = site_missing.groupby('SITE_ID')['gpp_missing'].sum().reset_index()\n",
    "\n",
    "gpp_missing_counts.rename(columns={'gpp_missing': 'gpp_counts'}, inplace=True)\n",
    "\n",
    "#merge missing predictive vars and gpp dfs\n",
    "sites_missing_counts = pd.merge(sites_missing_counts, gpp_missing_counts, on='SITE_ID', how='left')\n",
    "\n",
    "total_site_miss=sites_missing_counts['counts'].sum()\n",
    "total_site_miss #145071\n",
    "\n",
    "# calculate missing percentages\n",
    "sites_missing_counts['percentage']=(sites_missing_counts['counts']/total_site_miss)*100\n",
    "\n",
    "total_gpp_miss=sites_missing_counts['gpp_counts'].sum()\n",
    "\n",
    "sites_missing_counts['gpp_perc']=(sites_missing_counts['gpp_counts']/total_gpp_miss)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e310384",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "bars=plt.bar(sites_missing_counts['SITE_ID'], \n",
    "             sites_missing_counts['counts'],\n",
    "             color='red',edgecolor='black',\n",
    "             label='Total missing count',\n",
    "            alpha=0.69)\n",
    "\n",
    "gpp_bars = plt.bar(sites_missing_counts['SITE_ID'], sites_missing_counts['gpp_counts'], \n",
    "                   color='lightgreen', \n",
    "                   edgecolor='black', \n",
    "                   label='Missing GPP count',\n",
    "                   hatch='//',alpha=0.8)\n",
    "\n",
    "\n",
    "# percentage of total missing \n",
    "\n",
    "for bar, percentage in zip(bars,sites_missing_counts['percentage']):\n",
    "    yval=bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.05, f'{percentage:.1f}%',\n",
    "             ha='center', va='bottom',fontweight='bold')\n",
    "\n",
    "    \n",
    "# percentage of gpp missing\n",
    "\n",
    "for bar, gpp_percentage in zip(gpp_bars, sites_missing_counts['gpp_perc']):\n",
    "    if gpp_percentage > 0:  # Only show percentage if it's greater than 0\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.05, f'{gpp_percentage:.1f}%',\n",
    "                 ha='center', va='bottom', color='white',fontweight='bold')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "plt.xlabel('Site ID',fontsize=13)\n",
    "plt.ylabel('NA value count',fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87507fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###create a loop to extract all of the NA values per site \n",
    "\n",
    "site_na_dict={}\n",
    "\n",
    "for site_id in final_df['SITE_ID'].unique():\n",
    "    \n",
    "    site_df=final_df[final_df['SITE_ID']==site_id].copy()\n",
    "    \n",
    "    site_na_df=site_df[site_df.isna().any(axis=1)].copy()\n",
    "    \n",
    "    globals()[f\"{site_id.replace('-','_')}_NA\"]=site_na_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ed791",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_na_info = []\n",
    "\n",
    "# loop through each unique site id and process for rows with NA values\n",
    "for site_id in final_df['SITE_ID'].unique():\n",
    "    \n",
    "    site_df = final_df[final_df['SITE_ID'] == site_id].copy()\n",
    "    \n",
    "    # extract rows with NA values\n",
    "    site_na_df = site_df[site_df.isna().any(axis=1)].copy()\n",
    "    \n",
    "    # extract missing value information\n",
    "    for idx, row in site_na_df.iterrows():\n",
    "        missing_columns = row[row.isna()].index.tolist()\n",
    "        for col in missing_columns:\n",
    "            site_na_info.append({\n",
    "                'SITE_ID': site_id,\n",
    "                'year': row['YEAR'],\n",
    "                'month': row['MONTH'],\n",
    "                'day': row['DAY'],\n",
    "                'variable': col\n",
    "            })\n",
    "\n",
    "missing_info_df = pd.DataFrame(site_na_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check all of the sites for which missing data needed and the dates (this information is in the report appendix)\n",
    "\n",
    "site_id_to_check = 'US-PFa'\n",
    "filtered_df = missing_info_df[missing_info_df['SITE_ID'] == site_id_to_check]\n",
    "unique_variables = filtered_df['variable'].unique()\n",
    "unique_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9eb714",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique variables with missing values for SITE_ID '{site_id_to_check}':\")\n",
    "for variable in unique_variables:\n",
    "    print(f\"\\nVariable: {variable}\")\n",
    "    variable_dates = filtered_df[filtered_df['variable'] == variable][['year', 'month', 'day']]\n",
    "    print(variable_dates.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('/Users/abigailbase/PROJECT FILES/Pre imputation data/pre_imp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94842b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_key_selected.to_csv('/Users/abigailbase/PROJECT FILES/selected_sites.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
